\documentclass[inz, english, shortabstract]{iithesis}
%
\usepackage[utf8]{inputenc}
%
\polishtitle    {Implementacja statycznej i dynamicznej semantyki rachunku z efektami algebraicznymi i ich obsługą z pomocą biblioteki \Redex}
\englishtitle   {Implementation of static and~dynamic semantics for a calculus with algebraic effects and~handlers using \Redex} 
\polishabstract {Abstrakt w języku polskim}
\englishabstract{English abstract}
%
\author         {Maciej Buszka}
%
\advisor        {dr hab. Dariusz Biernacki}
%\date          {}                     % Data złożenia pracy
% Dane do oświadczenia o autorskim wykonaniu
%\transcriptnum {}                     % Numer indeksu
\advisorgen     {dr. hab. Dariusza Biernackiego}
\let\lll\undefined
\usepackage{csquotes, amsmath, amssymb, biblatex, float, ebproof}
%
\usepackage[pdftex]{graphicx}
\addbibresource{mybib.bib}
%
\floatstyle{boxed} 
\restylefloat{figure}
%
\newcommand{\Redex}{\texttt{PLT Redex} }
\newcommand{\Racket}{\texttt{Racket} }
\newcommand{\LC}{\(\lambda\)-calculus }
%
\begin{document}
%
\chapter{Introduction}
% dlaczego:
% experimentation with algebraic effects (calculus with various features)
% 
% lightweight prototyping
%
% goals:
% calculus
% implementation
% automatic testing (failed)
%
%
% What algebraic effects are, why are they interesting, why is it worth to experiment with different approaches/flavors
Algebraic effects \cite{Plotkin2003} are an increasingly popular technique of structuring computational effects.
They allow for seamless composition of multiple effects, while retaining (unlike monads) applicative style of programs.
Coupled with handlers \cite{Plotkin2013} which give programmers ability to interpret effects, they provide a great tool for abstracting over a set of operations which some program may perform and separating this interface from semantics of those operations defined as effect handlers.

% What are other approaches to algebraic effects
As these features are highly desirable many calculi and languages have been developed in order to get them just right; most notable of them being: \emph{Koka}\cite{Leijen2014} (featuring type inference, effect polymorphism with row-types and \emph{Javascript}-like syntax), \emph{Links}\cite{Hillerstrom2016} (featuring \emph{ML}-like syntax, row-typed effect polymorphism and ad-hoc effects) and \emph{Eff}\cite{Bauer2012} with implicit effect checking and recent work on direct compilation to \emph{OCaml}\cite{Kiselyov2018}.
On more theoretical side, various approaches to semantics of algebraic effects can be spotted in literature, both in respect to type system and run-time semantics.
Although most calculi use some form of row-types to implement tracking of effects there are differences in permitted shapes (at most one effect of given type or many effects), whether effects must be defined before use or not and how effects interact with polymorphism and abstraction.
At run-time handlers can wrap the captured continuation (giving so-called deep handlers) or not (shallow handlers) and the very act of finding the right handler can be implemented in various ways, mainly depending on some constructs which skip handlers.

% What am I implementing, what is interesting in this thesis, how it differs from other systems/languages (The main goal of the thesis)
All this variety naturally invites Us to experiment with different features and components of a calculus.
In this thesis I will build such a calculus, describing and justifying my choices and discussing the trade-offs I faced.
In order to rapidly iterate on design and test the calculus, I decided to use \Redex library which allows for building language model with executable type system judgments and reduction relation.
As such the other goal of my thesis was to assess viability of \Redex for development of bigger calculi.
To briefly summarize my development, the calculus consists of: 
\begin{itemize}
  \item Curry style type system with ad-hoc effects in the style of \emph{Links}, effect rows based on \emph{Koka} and \emph{lift} construct first shown in \cite{Biernacki2017}, implemented as unification based type inference algorithm.
  \item Executable reduction semantics most similar to system of \cite{Biernacki2017}.
  \item CEK style abstract machine with stack and \emph{meta}-stack of handlers
  \item Language front-end which translates human-friendly programs to calculus terms, integrated with \Racket environment, which allows for easy experimentation.
\end{itemize}

% Outline of the thesis
The rest of this thesis is structured as follows: in \autoref{ch:calculus} I describe the calculus in greater detail, in \autoref{ch:implementation} I discuss technicalities of implementation, in \autoref{ch:racket} I summarize the process of integration with \Racket environment and \autoref{ch:manual} is user's manual.
In the reminder of this chapter I introduce main topics of this thesis.

\section{Algebraic effects and handlers}
Algebraic effects and handlers are a language level framework which allow for coherent presentation, abstraction, composition and reasoning about computational effects.
The key idea is to separate invocation of an effectful operation in an expression from the meaning of such operation.
When one invokes an operation, current continuation (up to nearest handler) is captured and passed along with operation's argument to nearest handler.
The handler in turn may execute arbitrary expression, using the continuation once, twice, returning a function which calls the continuation or simply ignoring it.
This way many control structures can be modeled and generalized by algebraic effects and appropriate handlers.
For example, the exceptions can be modeled using a single operation \texttt{Throw} and a handler which either returns the result when computation succeeded or returns default value, ignoring passed continuation.
\begin{verbatim}
handle e with
| Throw () r -> // return default value
| return x   -> x
end
\end{verbatim}
From the language design standpoint algebraic effects provide single implementation of various phenomena which may happen during execution of a program, for example mutable state, I/O, environment lookup, exceptions etc. in a sense that every effect is treated the same, the typing rules are defined for invocation of any operation, and handling of any operation.
Similarly the operational semantics is also quite simple and succinct thanks to uniform treatment of various effects.
This framework is also extendable. With small additions if can handle built-in effects in addition to user-defined ones.

From the language user perspective algebraic effects provide means of abstraction over effects used in a program.
Thanks to easy creation of new effects, one can define special purpose operations and their handlers to better represent domain specific problems while simultaneously using well known effects, defined in standard library.
With effects being tracked by the type system, programmers can enforce purity or specific set of used effects at compile-time, or using effect polymorphism they can write reusable functions which abstract over effects which may happen.
The separation of definition and implementation of effects allows for various interpretations of operations, similar to a technique of \emph{dependency-injection} used for example during testing.
% TODO anonymous vs defined effects, abstract effects, effect polymorphism

\section{Type inference}
Type inference is a technique of algorithmic reconstruction of types for various constructions used in a language.
It allows programmers to write programs with no type annotations, which often feel redundant and obfuscate the meaning of a program.
The most well known type system with inference is a system for \emph{ML} family of languages - \emph{Haskell}, \emph{OCaml}, \emph{SML} which infers the types with no annotations whatsoever.
Formal type system defines grammar of types consisting of base types (\texttt{int}, \texttt{bool} etc.), type constructors (arrows, algebraic data types) and type variables.
The typing rules require types which should be compatible (f.e. formal parameter and argument types) to unify.
The key feature of this system is so called let-polymorphism - generalization of types of let-bound variables.
This way code reuse can be accomplished without complicating the type system and compromising type safety.
The basis of implementation of this system is first order unification algorithm, which syntactically decomposes types and builds a substitution from type variables to types.

\section{Reduction semantics and abstract machines}
% TODO add reference to canonical definitions for good definition of reduction semantics and abstract machines, especially CEK
Reduction semantics is a format for specifying dynamic semantics of a calculus in an operational style.
The basic idea is to first define redexes - expressions which can be reduced, and contexts in which the reduction can happen.
Taking \LC with call-by-value reduction order as an example, the only redex is application of a function to value $ (\lambda x . e) v $.
The possible contexts are: empty context $ \square $ or evaluation of operator part of application $ E e $ or evaluation of operand $ v E $.
\begin{figure}
  \includegraphics{lc-syntax.pdf}
  \caption{\LC abstract syntax}
  \label{fig:lc-syntax}
\end{figure}
With these possibilities in mind, we will define binary relation $ \longrightarrow $ which describes single step of reduction.
Such relation can be thought of as a transition system, rewriting terms into simpler ones step by step.
There usually are two approaches to definition of such relation:
\begin{itemize}
  \item Definition of primitive reduction $ (\lambda x . e) v \longrightarrow_p e\{v/x\} $ which operates only on redexes and giving it a closure via following inference rule:
  \begin{prooftree}
    \Hypo{e \longrightarrow_p e'}
    \Infer1{E[e] \longrightarrow E[e']}
  \end{prooftree},
  which says that if we can primitively reduce some expression, than we can do it in any context.
  \item Or definition of $ \longrightarrow $ directly, with decomposition of terms on both sides: $ E[(\lambda x . e) v] \longrightarrow E[e\{v/x\}] $
\end{itemize}
where the syntax $ e\{v/x\} $ means term $ e $ with value $ v $ substituted for variable $ x $, and $ E[e] $ means some context $ E $ with expression $ e $ inserted into the hole.
For both approaches it is important, that any term can be uniquely decomposed into redex and context, because when it is the case, then the relation is deterministic and gives good basis for formulation of abstract machines, interpreters or transformations to some other intermediate representations.
\begin{figure}
  \includegraphics{lc-red.pdf} 
  \caption{\LC reduction relation}
  \label{fig:lc-red}
\end{figure}
\begin{figure}
  \LC reduction example
  \caption{\LC example reduction sequence}
  \label{fig:lc-red-example}
\end{figure}

Abstract machine is a mathematical construction, usually defined as a set of configurations with deterministic transformations, which are computationally simple.
The goal for formulation of an abstract machine is to mechanize evaluation of terms while retaining semantics given in more abstract format, f.e. reduction semantics, with the correspondence being provable\cite{Felleisen2009}.
As an example I will show a \emph{CEK}-machine for the \LC defined earlier.
The name \emph{CEK} comes from \emph{C}ommand, \emph{E}nvironment and \emph{K}ontinuation.
The machine configuration is a triple $ (e, \rho, \kappa) $ where $ e $ is an expression which is decomposed or reduced, $ \rho $ is an environment mapping variables to values, and the last component $ \kappa $ is a continuation stack, which determines what will happen with value, to which first component eventually reduces.
Thanks to the environment we no longer have to explicitly perform substitution, leading to more machine friendly and efficient implementation.
Given an initial state, the machine can then repeatedly apply transformation relation, either looping, arriving at a final value, or getting stuck. 
\begin{figure}
  \emph{CEK}-machine for \LC
  \caption{\LC abstract machine}
  \label{fig:lc-cek}
\end{figure}

\section{\Redex}
% TODO: Redex section

\chapter{The calculus}\label{ch:calculus}
The calculus implemented in this thesis is based on lambda calculus with call-by-value semantics.
This choice follows other calculi which allow for computational effects, because fixed evaluation order is essential to obtaining sane program semantics.
Inspired by \emph{Links} \cite{Hillerstrom2016} the operations are truly ad-hoc meaning that they don't have to be declared before usage.
Moreover the calculus requires no type annotations whatsoever in spirit of \emph{ML} family of languages while still tracking effects which occur in a program.
The \LC is extended with base types (Numeric and Boolean) with corresponding operations and literals, conditional expression and recursive functions.
Effectful operations are invoked similarly to function calls (although they are a distinct syntactic category) and are handled using \textit{handle} construct, they may also be lifted with \textit{lift} syntactic form.
Usually the operation will be handled by the closest handler with appropriate case, unless there is a lift for this operation in between, each one causing operation to skip one handler.
Abstract syntax of the calculus is presented in \autoref{fig:algeff-syntax}

\begin{figure}
  \centering
  \includegraphics{algeff-syntax.pdf}
  \caption{Abstract syntax}
  \label{fig:algeff-syntax}
\end{figure}

\section{Static semantics}
The type system is based on \emph{Koka}(Leijen's style of row types\cite{Leijen2005}), \emph{Links}\cite{Hillerstrom2016} (ad-hoc operations) and Biernacki et al. \cite{Biernacki2017} (lift construct) systems.
Initially I implemented a variant of System-F extended with row-types but it proved to be a bit of a mouthful to write even simplest programs.
Moreover the \texttt{Redex}'s facilities for automatic testing were not able to generate well typed terms, so some type inference was inevitable.
To limit the amount of work I decided to present the calculus in Curry style, with typing relation inferring the type for unannotated terms.
Building on well known foundations, types are inferred via first order unification.
The system does not feature polymorphism in first class fashion, as there is no rule where types are generalized, but I believe it to be a straightforward addition, following the \emph{Koka}\cite{Leijen2014} calculus.
Still, after inferring type of an expression, we can see which unification variables are left abstract and could be generalized.
There are two main features differentiating this system from \emph{Koka}'s; firstly effects need not be defined before use, their signature is inferred as with any other construction; secondly the system is algorithmic, with rules explicitly encoding a recursive function which can infer the type of an expression.

\subsection{Row-types}
Row types as described in \cite{Leijen2005}

\subsection{Type inference}
The judgment $ \Gamma \mid [S_1 \, N_1] \vdash e \, : \, t \, ! \, row \mid [S_2 \, N_2] $ asserts that in typing context $ \Gamma $ under type substitution $ S_1 $, with name supply state $ N_1 $ expression $ e $ has type $ t $ with effects $ row $ under type substitution $ S_2 $ and with name supply state $ S_2 $.
Algorithmically this judgment infers a type and effect row, and calculates new substitution, given typing environment, current substitution and an expression.
As in \emph{ML} languages only simple types can be inferred, along with effect rows.
Base rules for constants and variable lookup are straightforward, each introducing fresh effect row variable.
To check $ \lambda $ expression, we first introduce fresh type variable, and then check the body in extended environment.
The arrow gets annotated with effects which may occur during evaluation of the body and the $ \lambda $ abstraction itself is returned with fresh effect row.
The recursive function checking is similar to normal functions.
First variable denotes function itself, while second it's argument.
Accordingly the environment gets extended with functional type $ t_1 \rightarrow row_1 t_2 $ to check the body of the function, and afterwards the result type of body $ t $ gets unified with the result type of function $ t_2 $, same with effect row.
Whole function, as it is a value, is returned with fresh effect row.
The application requires expression at function position to be of functional type and parameter type to unify with argument type.
All effect rows (from evaluation of function value, argument value and function body) must unify as well.
The checking of primitive operation gets deferred to auxiliary judgment, which checks arity and argument types, returning result type and usually fresh effect row.
Conditional expression requires the condition expression to be of type $ Bool $ and types of two branches to unify.
As usual all effect rows must also unify.
Operation invocation requires the effect row to contain operation $ op $ whose input type is the inferred type for $ e $ and output type being fresh.
Operation lifting prepends fresh $ op $ to the effect row of $ e $.
Finally, to check handle expression we first infer the type of enclosed expression $ e $, then in environment extended with $e$'s type $ t_1 $ we infer return expression's type $ t_ret $.
Helper judgment \textit{infer-handlers} returns the result effect row of handlers $ row_{out} $ and row marking handled effects $ row_{handled} $ whose tail is the same as result's.
By unifying result row with return row and handled row with $ row_1 $ we ensure that effects which may occur during handling of operations, at return and leftovers from the inner expression are all accounted for.


\begin{figure}
  \centering
  \includegraphics{algeff-infer.pdf}
  \caption{Type system} 
  \label{fig:algeff-infer} 
\end{figure}

\subsection{Effect handlers}


\section{Reduction semantics}
\begin{figure}
  \centering
  \includegraphics{algeff-red.pdf}
  \caption{Reduction relation} 
  \label{fig:algeff-red}
\end{figure}


\section{Abstract machine}


\chapter{Implementation}\label{ch:implementation}

\section{\Redex}
% scaling problems

\section{Typing relation}

\section{Unification}

\section{Reduction relation}

\section{Automatic testing}


\chapter{The \Racket environment}\label{ch:racket}

\section{Front-end}

\section{Back-end}

% TODO implementation architecture
\chapter{User's manual}\label{ch:manual}

\printbibliography

%\begin{thebibliography}{1}
%\bibitem{example} \ldots
%\end{thebibliography}

\end{document}

\documentclass[inz, english, shortabstract]{iithesis}
%
% \usepackage[utf8]{inputenc} 
%
\polishtitle    {Implementacja statycznej i dynamicznej semantyki rachunku z efektami algebraicznymi i ich obsługą z pomocą biblioteki \Redex}
\englishtitle   {Implementation of static and~dynamic semantics for a calculus with algebraic effects and~handlers using \Redex} 
\polishabstract {Abstrakt w języku polskim}
\englishabstract{%
Algebraic effects are an increasingly popular technique of structuring computational effects.
They allow for separation of an interface of a computation -- a set of operations it may perform, and their implementation -- handlers which respond to operation invocations.
There are many calculi and languages that tackle the concept of algebraic effects \cites{Bauer2012,Leijen2014,Hillerstrom2016,Lindley2017, Biernacki2019}, but they usually are either specialized and developed using heavy-weight formalization, or they strive to become full-fledged programming languages with their meta-theory and implementation becoming increasingly complex.
In this thesis I set out to build a small calculus, that has just enough general purpose constructs to allow for exploration and experimentation with algebraic effects and handlers, without overburdening the user.
The meta-development consists of the Curry style type system, the reduction semantics and the abstract-machine.
The calculus is built using the \Redex{} library and allows the programmer to use operations and handlers, recursive functions, numbers, booleans and conditional expressions.
The implementation of the type system provides type inference for the calculus terms, while both the reduction relation and the abstract machine can be used to either fully evaluate the expressions or visualize the reductions step-by-step.
Additionally I present the process of integrating this development with the \Racket{} environment, that allows for usage of the calculus as a lightweight programming language.
}
%
\author         {Maciej Buszka}
%
\advisor        {dr hab. Dariusz Biernacki}
%\date          {}                     % Data złożenia pracy
% Dane do oświadczenia o autorskim wykonaniu
%\transcriptnum {}                     % Numer indeksu
\advisorgen     {dr. hab. Dariusza Biernackiego}
\let\lll\undefined
\usepackage{csquotes, amsmath, amssymb, biblatex, float, ebproof}
\usepackage{minted}
%
\usepackage[xetex]{graphicx}
\addbibresource{mybib.bib}
%
\floatstyle{boxed} 
\restylefloat{figure} 
%
\newcommand{\Redex}{\texttt{PLT Redex}}
\newcommand{\Racket}{\texttt{Racket}}
\newcommand{\LC}{\(\lambda\)-calculus}
\mathchardef\mhyphen="2D % Define a "math hyphen"
\def\listingautorefname{Listing}
%
\begin{document}
%
\chapter{Introduction}
% dlaczego:
% experimentation with algebraic effects (calculus with various features)
% 
% lightweight prototyping
%
% goals:
% calculus
% implementation
% automatic testing (failed)
%
%
% What algebraic effects are, why are they interesting, why is it worth to experiment with different approaches/flavors
Algebraic effects \cite{Plotkin2003} are an increasingly popular technique of structuring computational effects.
They allow for seamless composition of multiple effects, while retaining (unlike monads) applicative style of programs.
Coupled with handlers \cite{Plotkin2013} which give programmers ability to interpret effects, they provide a disciplined and flexible tool for abstracting over a set of operations which a program may perform and for separating this interface from the semantics of those operations, defined as effect handlers.

% What are other approaches to algebraic effects
As composability and separation of concerns are often sought after, many calculi and languages have been developed in order to get algebraic effects just right; most notable of them being: \emph{Koka} \cite{Leijen2014} (featuring type inference, effect polymorphism with row-types and \emph{JavaScript}-like syntax), \emph{Links} \cite{Hillerstrom2016} (featuring \emph{ML}-like syntax, row-typed effect polymorphism and ad-hoc effects), \emph{Helium} \cite{Biernacki2019} (with abstract and local effects, \emph{ML}-style module system and principled approach to effect polymorphism), \emph{Eff} \cite{Bauer2012} (with implicit effect checking and recent work on direct compilation to \emph{OCaml} \cite{Kiselyov2018}) and \emph{Frank} \cite{Lindley2017} (with bidirectional type and effect system requiring minimal amount of effect variables, and shallow effect handlers). 

On a more theoretical side, various approaches to semantics of algebraic effects can be spotted in the literature, both with respect to type systems and run-time semantics.
Although most calculi use some form of row-types (with notable exception of \emph{Frank} \cite{Lindley2017}) to track effects, there are differences in permitted shapes (at most one effect \cite{Hillerstrom2016} of given type or many effects \cite{Biernacki2017,Leijen2014}, whether effects must be defined before use \cite{Biernacki2017,Lindley2017,Leijen2014,Bauer2012} or not \cite{Hillerstrom2016}) and how effects interact with polymorphism and abstraction.
At run-time handlers can wrap the captured continuation (giving so-called deep handlers \cite{Biernacki2017,Hillerstrom2016,Leijen2014,Bauer2012}) or not (shallow handlers \cite{Lindley2017}) and the very act of finding the right handler can be implemented in various ways, mainly depending on some constructs which skip handlers \cite{Biernacki2017}.

% What am I implementing, what is interesting in this thesis, how it differs from other systems/languages (The main goal of the thesis)
% Goals:
%  runnable calculus with algebraic effects
%  user friendly (usable, machine executable, algorithmic) type system
%  introspectable reductions
%  runnable abstract machine
%  language front-end facilitating ease of experimentation
%  some lightweight machine checking (automatic tests)
All this variety naturally invites us to experiment with different features and components of a calculus.
An executable model would be excellent for everyone interested in understanding inner workings of algebraic effects.
In particular, the ability to perform and visualize reductions would be very helpful for understanding of the dynamic semantics of operations and handlers.
Such model should also contain a type system complementing the dynamic semantics and guiding the programmer with usage of effects.
In this thesis I will build such a calculus, describing my choices and discussing the trade-offs I faced.

The first goal of this thesis is the design of the calculus with effects and handlers, and its dynamic semantics.
Its implementation should be executable and allow for step-by-step reduction of calculus terms by a computer program.

The second goal is the design and implementation of a sound type system for the calculus, preferably with type inference.
The implementation should be able to type-check examples with minimal additional context and boilerplate.

The third goal is the design and implementation of abstract machine, which preserves the dynamic semantics of the calculus, yet is easier to translate into low-level virtual machine.
It could also be used as a basis for compilation to native code.

In order to rapidly iterate on the design and test the calculus, I decided to use the \Redex{} library which allows for building language model with executable type system judgments and reduction relation.
It also provides facilities for visualizing rewriting of terms and typesetting all components of the development.

The calculus is designed to be small enough to be easily understood, yet have general language features to allow experimentation with reasonably complex programs.
The programmer can use $\lambda$-abstractions and recursive functions, numbers with addition, subtraction, multiplication and comparisons, booleans with conditional expressions and lists.
The algebraic effects are implemented with ad-hoc operations which take one parameter and return a value, handlers which can handle multiple (different) operations at once and lifts which allow operations to skip handlers.
The handler wraps captured continuation, giving the deep handling semantics.
The type system for calculus is presented in Curry style, with the implementation inferring simple types for unannotated terms using unification algorithm.
Although there is no way to create and bind polymorphic values, the system infers most general (simple) type for an expression which may contain unsolved variables.
The abstract machine is implemented with explicit stack of continuations and value environment.
It is given deterministic transition system using \Redex{}'s reduction relation and meta-function transforming a calculus expression into initial configuration.
Additionally to ease experimentation I implemented a language front-end which translates human-friendly programs to calculus terms, integrated with \Racket{} environment.

In brief summary, the development consists of:
\begin{itemize}
  \item Executable reduction semantics most similar to the system of \cite{Biernacki2017}.
  \item Curry style type system with ad-hoc effects in the style of \emph{Links}, effect rows based on \emph{Koka} and \emph{lift} construct of \cite{Biernacki2017}, implemented as a unification-based type inference algorithm.
  \item CEK style abstract machine with stack and \emph{meta}-stack of handlers, based on \cite{Hillerstrom2016}
\end{itemize}

% Outline of the thesis
The rest of this thesis is structured as follows: in the remainder of this chapter I introduce the main topics of this thesis, in \autoref{ch:calculus} I describe the calculus in greater detail, in \autoref{ch:implementation} I discuss technicalities of implementation and integration with the \Racket{} environment and \autoref{ch:manual} is user's manual.

\section{Algebraic effects and handlers}
Algebraic effects and handlers are a language level framework which allow for coherent presentation, abstraction, composition and reasoning about computational effects.
The key idea is to separate invocation of an effectful operation in an expression from the meaning of such an operation.
When one invokes an operation, current continuation (up to the nearest handler) is captured and passed along with the operation's argument to the nearest handler.
The handler in turn may execute arbitrary expression, using the continuation once, twice, returning a function which calls the continuation or simply ignoring it.
This way many control structures can be modeled and generalized by algebraic effects and appropriate handlers.
For example, in the following \autoref{lst:exception-like}, function \texttt{exists} returns \texttt{true} when the list contains an element that satisfies predicate \texttt{p}.
\begin{listing}[H]
  \VerbatimInput[firstline=13,lastline=19]{../algeff/test/05.rkt}
  \caption{Exception-like usage of algebraic effects}
  \label{lst:exception-like}
\end{listing}
It is implemented in terms of map and a helper function \texttt{f} which \texttt{Break}s normal control flow when the predicate returns true.
This map is then invoked inside the handler that returns \texttt{true} on \texttt{Break} and \texttt{false} otherwise.
This usage of operation and its handler is similar to exceptions, as the resumption is discarded.
Another example, with a handler for the state-like operations is presented in \autoref{lst:stateful-computation} in the next section.

From the language design standpoint algebraic effects provide single implementation of various phenomena which may happen during execution of a program, for example mutable state, I/O, environment lookup, exceptions, etc., in a sense that every effect is treated the same, the typing rules are defined for invocation of any operation, and handling of any operation.
Similarly the operational semantics is also quite simple and succinct thanks to uniform treatment of various effects.
This framework is also extendable.
With small extension it can handle built-in effects in addition to user-defined ones.

From the language user perspective algebraic effects provide means of abstraction over effects used in a program.
Thanks to easy creation of new effects, one can define special purpose operations and their handlers to better represent domain specific problems while simultaneously using well known effects, defined in the standard library.
With effects being tracked by the type system, programmers can enforce purity or specific set of used effects at compile-time, or using effect polymorphism they can write reusable functions that abstract over effects which may happen.
The separation of definition and implementation of effects allows for various interpretations of operations, for example simulating a database connection or file-system during testing.
% TODO anonymous vs defined effects, abstract effects, effect polymorphism

\section{Types and type inference}
The most common approach to giving an effectful computation a type uses a type-level data-structure known as a row.
Initially developed in order to structurally type records, rows come in two flavors: Remy-style\cite{Remy1994} where they are treated as (finite) sets of label-type pairs, and Leijen-style\cite{Leijen2005} where rows are treated as (finite) lists of label-type pairs that are equivalent up to permutation of different labels.
When polymorphism is present a row may have concrete prefix (possibly empty) and polymorphic tail denoted by a type variable.
In effectful setting, the type system usually keeps a row of effects which an expression may perform, and suspended computations e.g. functions must have types decorated with a row of operations that may be invoked when their body is evaluated.
In the following listing, the function \texttt{add} loads a number, sums it with the argument \texttt{x} and sets this sum, returning unit value.
\begin{listing}[H]
  \begin{verbatim}
let add = λ x Set +(Get (), x) in
let comp = 
  handle add 5 with
  | Set x r -> λ _ r () x
  | Get _ r -> λ s r s s
  | return _ -> λ s s
  end in
comp 37
  \end{verbatim}
  \caption{Stateful computation}
  \label{lst:stateful-computation}
\end{listing}
It contains two effectful sub-expressions: $Get \, : \, Unit \Rightarrow Num$ and $Set \, Num \Rightarrow Unit$.
As this function uses both operations, their effects must be combined, giving add following type: $Num \rightarrow (Get \, : \, Unit \Rightarrow Num , Set \, : \, Num \Rightarrow Unit) \, Unit$ where $Num$ is the type of input, $Unit$ is the type of output and $(Get \, \ldots)$ is the effect row.
The handler which interprets the operations is implemented to return a state transforming function.
When the \texttt{Set} operation is invoked, the handler returns a function which will ignore its argument and first resume the computation with unit value, that will return a state transformation and then pass it the new state \texttt{x}.
The \texttt{Get} operation handler returns a function which awaits for a state $s$ with which it resumes the continuation and then applies returned function to $s$.
The return clause returns identity state transformation.
The type system requires all handler bodies to be of the same type: $Num \rightarrow () \, Num$ which guides implementation and ensures that operations do not escape the handler.

Type inference is a technique of algorithmic reconstruction of types for various constructions used in a language.
It allows programmers to write programs with no type annotations, that often feel redundant and obfuscate the meaning of a program.
The most well known type system with inference is a system for \emph{ML} family of languages\cite{Pierce2002} -- \emph{Haskell}, \emph{OCaml}, \emph{SML} which infers the types with no annotations whatsoever.
A formal type system defines grammar of types consisting of base types (\texttt{int}, \texttt{bool} etc.), type constructors (arrows, algebraic data types) and type variables.
The typing rules require types which should be compatible (e.g. formal parameter and argument types) to unify.
The key feature of this system is the so-called let-polymorphism -- generalization of types of let-bound variables.
This way code reuse can be accomplished without complicating the type system and compromising type safety.
The basis of implementation of this system is first order unification algorithm\cite{Pierce2002}, which syntactically decomposes types and builds a substitution from type variables to types.



\section{Reduction semantics and abstract machines}
% TODO add reference to canonical definitions for good definition of reduction semantics and abstract machines, especially CEK
Reduction semantics\cite{Felleisen2009} is a format for specifying dynamic semantics of a calculus in an operational style.
The basic idea is to first define redexes -- expressions which can be reduced, and contexts in which the reduction can happen.
Taking \LC{} extended with numbers (\autoref{fig:lc-syntax}) and with call-by-value reduction order as an example, the only redex is application of a function to value $ (\lambda x . e) v $ as shown in \autoref{fig:lc-red}.
The possible contexts are: empty context $ \square $ or evaluation of operator part of application $ K e $ or evaluation of operand $ v K $ when left part has already been evaluated to a value.
\begin{figure}
  \includegraphics{lc-syntax.pdf}
  \caption{\LC{} abstract syntax}
  \label{fig:lc-syntax}
\end{figure}
With these possibilities in mind, we will define binary relation $ \longrightarrow $ which describes single step of reduction.
Such relation can be thought of as a transition system, rewriting terms into 'simpler' ones step by step.
There usually are two approaches to definition of such relation:
\begin{itemize}
  \item Definition of primitive reduction $ (\lambda x . e) v \longrightarrow_p e\{v/x\} $ which operates only on redexes and giving it a closure with the following inference rule:
  
  \begin{prooftree}
    \Hypo{e \longrightarrow_p e'}
    \Infer1{K[e] \longrightarrow K[e']}
  \end{prooftree}

  which says that if we can primitively reduce some expression, then we can do it in any context.
  \item Or definition of $ \longrightarrow $ directly, with decomposition of terms on both sides: $ K[(\lambda x . e) v] \longrightarrow K[e\{v/x\}] $
\end{itemize}
where the syntax $ e\{v/x\} $ means term $ e $ with value $ v $ substituted for variable $ x $, and $ K[e] $ means some context $ K $ with expression $ e $ inserted into the hole.
For both approaches it is important, that any term can be uniquely decomposed into redex and context, because when it is the case, then the relation is deterministic and gives good basis for formulation of abstract machines, interpreters or transformations to some other intermediate representations.
\begin{figure}
  \includegraphics{lc-red.pdf} 
  \caption{\LC{} reduction relation}
  \label{fig:lc-red}
\end{figure}

Abstract machine is a mathematical construction, usually defined as a set of configurations with deterministic transformations, which are computationally simple.
The goal for formulation of an abstract machine is to mechanize evaluation of terms while retaining semantics given in a more abstract format, e.g. reduction semantics, with the correspondence being provable\cite{Felleisen2009}.
As an example, \autoref{fig:lc-cek} shows a \emph{CEK}-machine for the \LC{} defined earlier.
The name \emph{CEK} comes from \emph{C}ommand, \emph{E}nvironment and \emph{K}ontinuation.
The machine configuration is a triple $ (e, \rho, \kappa) $ where $ e $ is an expression which is decomposed or reduced, $ \rho $ is an environment mapping variables to values, and the last component $ \kappa $ is a continuation stack, which determines what will happen with value, to which first component eventually reduces.
Thanks to the environment we no longer have to explicitly perform substitution, leading to more machine friendly and efficient implementation.
Given an initial state, the machine can then repeatedly apply transformation relation, either looping, arriving at a final value, or getting stuck. 
\begin{figure}
  \emph{CEK}-machine for \LC{}
  \caption{\LC{} abstract machine}
  \label{fig:lc-cek}
\end{figure}

\section{\Redex}
The \Redex{}\cite{Felleisen2009} library provides a comprehensive set of tools for the development of various calculi and language-like artifacts.
The work begins with the definition of a language using a familiar BNF-like syntax.
The library provides many options for defining patterns which describe the abstract syntax of the language, among them: meta-variables, symbols -- playing the role of markers, numbers, object language variables, repetitions of patterns using ellipsis and nonlinear patterns which can for example enforce that all variables in a binding are different.
Besides the syntax, the language definition allows for specifying the variable binding structure of the object language, which will be used by built-in meta-functions for substitution.
The following \autoref{lst:lc-typed-syntax-code} shows code extending \LC{} defined earlier in \autoref{fig:lc-syntax} with typed terms $ E $, along with types $ t $ and typing contexts $ \Gamma $.
\begin{listing}[H]
  \inputminted[firstline=26, lastline=32]{Racket}{../lc/lc.rkt}
  \caption{Typed \LC{} with numbers in \Redex{}}
  \label{lst:lc-typed-syntax-code}
\end{listing}
Second feature of the \Redex{} library are meta-functions which can pattern match on terms and return other terms.
They may use full power of complex and non-linear patterns which the library exposes, additional side conditions and even escape to \Racket{} -- the host language in which the \Redex{} is defined.

\begin{listing}[H]
  \inputminted[firstline=34,lastline=51]{Racket}{../lc/lc.rkt}
  \caption{Type system for \LC{} in \Redex{}}
  \label{lst:lc-typed-type-code}
\end{listing}

The third aspect of this library are judgment forms which encode inductively defined judgments (e.g. type systems) with a syntax similar to pen-and-paper rules. \autoref{lst:lc-typed-type-code} shows example code, defining a simple type system for annotated \LC{} with numbers.
These rules can use patterns, meta-functions, other judgments and also escape to \Racket{}.
When defining a judgment, the programmer must specify which parameters are to be treated as inputs and which as outputs using \mintinline{Racket}{#:mode} keyword.
The library enforces an invariant that inputs \texttt{I} of a judgment form must be concrete terms and outputs \texttt{O} may be variables.
Under the hood the \Redex{} library will resolve the rules in depth-first order backtracking on failures and multiple pattern matches.

The last component of language modeled using \Redex{} are reduction relations, usually used for specifying semantics.
They are defined as a set of clauses, which should rewrite an input term into other term of same syntactic category.
In order to find redex, the programmer can define evaluation contexts, and then the library will decompose the terms using \mintinline{Racket}{(in-hole K e)} pattern, as in \autoref{lst:lc-red-code}.

\begin{listing}[H]
  \inputminted[firstline=19,lastline=23]{Racket}{../lc/lc.rkt}
  \caption{Reduction relation for \LC{} in \Redex{}}
  \label{lst:lc-red-code}
\end{listing}

Finally \Redex{} provides features for automatic testing via term generation facilities and has ability to typeset every component of calculus development.
Every figure in this thesis, which shows language grammar, reduction relation, meta-function or judgment has been generated using \Redex{}.

\chapter{The calculus}\label{ch:calculus}
The calculus implemented in this thesis is based on \LC{} with call-by-value semantics.
Its abstract syntax is presented in \autoref{fig:algeff-syntax}.
Meta-variable $ x $ ranges over variables used in value binders and their references, while $ op $ ranges over operation names, which are distinct from normal variables.
Meta-variable $ v $ ranges over values, which are one of: boolean $ b $, number $ m $, $ \lambda $-abstraction $(\lambda \, x \, e)$, recursive function $(rec \, x_f \, x_v \, e)$ or a list of values $(v \ldots)$.
Meta-variable $ e $ ranges over expressions, which include values $ v $, forms standard to \LC{} -- variables $ x $, function applications $ (e \, e) $, conditionals $ (if \, e \,e \, e) $ and primitive operations $ (prim \, e \ldots) $; they also include three constructs specific to effects -- operation invocations $ (op \, e) $, lifts $ (lift \, op \, e) $ and handlers $ (handle \, e \, hs \, ret) $ where $ ret $ is return expression $ (return \, x \, e) $ and $ hs $ is a list of handler clauses.

To achieve call-by-value, left-to-right reduction order I use evaluation contexts $ E $; this choice follows other calculi which allow for computational effects\cite{Biernacki2017, Leijen2014, Hillerstrom2016}.
One interesting aspect of these contexts is notion of \emph{free}ness \cite{Biernacki2017}, defined in \autoref{fig:algeff-free}.
The judgment $ free[ \, op , \, E , \, n] $ asserts that operation $ op $ is $ n $-free in evaluation context $ E $, meaning that it will be handled by $(n + 1)$st handler for $ op $ \emph{outside} the context $ E $.

The syntax of types, ranged over by meta-variable $ t $, comprises base types ($Int$, $Bool$), lists $List \, t$, arrow types $(t \rightarrow row \, t)$, operation types $(t \Rightarrow t)$, row types $row$ and type variables $ a $.
Rows are defined inductively as either an empty row $\cdot$, a variable $ a $ or an extension $(op \,\, t \,\, row)$ of a row $row$ with a type $t$ assigned to an operation label $op$, and are ranged over by meta-variable $row$.
Finally, meta-variable $\Gamma$ ranges over typing contexts, $S$ over type substitutions and $SN$ denotes a pair of substitution and name supply $N$ -- a natural number used to generate fresh type variables.

\begin{figure}
  \centering
  \includegraphics{algeff-syntax.pdf}
  \caption{Abstract syntax}
  \label{fig:algeff-syntax}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{algeff-free.pdf}
  \caption{Context \emph{free}ness}
  \label{fig:algeff-free}  
\end{figure}

\section{Dynamic semantics}
The dynamic semantics for a calculus with algebraic effects defines, besides the standard reductions known from \LC{}, the control structure of operations and handlers.
Intuitively, when an operation $op$ is invoked, it will be handled by dynamically closest handler, with a caveat that for each lift passed in search of handler, it must skip one handler.
Formally, when an operation $ op $ is invoked, it will be handled by lexically enclosing handler $ (handle \, E[op \, e] \, hs \, ret) $ if and only if the intermediate context $ E $ is $0$-free \cite{Biernacki2017}.

The dynamic semantics is defined in the format of contextual reduction semantics in \autoref{fig:algeff-red}.
All rules perform reduction in a context $ E $ leaving it unchanged.
The first rule $\beta\mhyphen\lambda$ describes standard $ \beta $-reduction via substitution of argument value for variable in function body, while the second $\beta\mhyphen rec$ is the $ \beta $-reduction of recursive function, where first we substitute the function for function variable and then substitute the argument.

The rule \emph{prim-op} deals with built-in primitive operations using helper meta-function \textit{prim-apply} which pattern matches on $prim$ and performs the appropriate operation.
Next two rules \emph{if-true} and \emph{if-false} perform a choice of the correct branch in a conditional expression depending on the value of the condition.
The rule \emph{lift-compat} returns a value from a lift expression, leaving it unchanged.

The rule \textit{handle-return} handles the case when the inner expression of a handle expression evaluates to a value, which means we have to evaluate the return clause by substituting the result value for $ x $, and plugging this expression into the evaluation context $ E $.

The last rule \textit{handle-op} describes the behavior when an expression calls some operation.
To handle an operation we must find a $0$-free inner context $ E_2 $ which is directly surrounded by a handle expression which has a case for $ op $.
Then we substitute the value $ v $ for the first variable $x_1$ of the operation handler and the inner context $ E_2 $ surrounded with the very same handler (the continuation delimited by the handler) closed in a lambda for the second argument $x_2$.
This wrapping gives deep handling semantics, as an operation call cannot escape from a handler in the resumption.
The operation handler can resume the evaluation of the expression which invoked the handled operation using the function bound by $x_2$.

\begin{figure}
  \centering
  \includegraphics{algeff-red.pdf}
  \caption{Reduction relation} 
  \label{fig:algeff-red}
\end{figure} 

\section{Static semantics}
The type system is based on \emph{Koka} \cite{Leijen2014} (Leijen's style of row types \cite{Leijen2005}), \emph{Links} \cite{Hillerstrom2016} (ad-hoc operations) and Biernacki et al's. \cite{Biernacki2017} (lift construct) systems.
Initially I implemented a variant of System F extended with row-types but it proved to be a bit of a mouthful to write in it even the simplest programs.
Additionally the \Redex{}'s facilities for generation of terms were not able to generate sufficiently many well typed ones.
As I intended on using the automated counterexample search to test assertions about the calculus, my goal was to minimize the amount of programs rejected by the type system.
In order to accomplish this goal, type inference proved to be very helpful as it increased the amount of well typed terms tenfold.

To limit the amount of work and keep the calculus reasonably simple, I decided to present the calculus in the Curry style, with the typing relation inferring the type for unannotated terms, instead of implementing a separate type system and an inference algorithm.

Building on well known foundations\cite{Pierce2002}, types are inferred via first-order unification.
While the actual algorithm is presented in \autoref{sec:unification}, the notion of unification is used extensively in the remainder of this chapter and as such I will present an intuitive definition here.
Two types $t_1$ and $t_2$ \emph{unify} (written $t_1 \sim t_2$) if they are structurally the same, where variables can be substituted with any type.
Two rows unify, if they are the same list of operation-type pairs, up to permutation of different operations.

The system does not feature polymorphism in a first-class fashion, as no polymorphic functions can be bound, mainly because there is no rule where types are generalized, but I believe it to be a straightforward addition, following the \emph{Koka} \cite{Leijen2014} calculus.
Still, after inferring the type of an expression, we can see which unification variables are left abstract and could be generalized.
There are two main features differentiating this system from \emph{Koka}'s; firstly effects need not be defined before use, their signature is inferred the same way as any other construction; secondly the system is algorithmic, with rules explicitly encoding a recursive function which can infer the type of an expression.

\subsection{Type inference}
The judgment $ \Gamma \mid [S_1 \, N_1] \vdash e \, : \, t \, ! \, row \mid [S_2 \, N_2] $ asserts that in a typing context $ \Gamma $ under a type substitution $ S_1 $, with name supply state $ N_1 $, expression $ e $ has type $ t $ with effects $ row $ under a type substitution $ S_2 $ and with a name supply state $ N_2 $.
Algorithmically this judgment infers a type and an effect row, and calculates new substitution, given typing environment, current substitution and an expression.
As in \emph{ML} languages only simple types can be inferred, along with effect rows.
The judgment rules are presented in \autoref{fig:algeff-infer}.

\begin{figure}
  \centering
  \includegraphics{algeff-infer.pdf}
  \caption{Type system} 
  \label{fig:algeff-infer} 
\end{figure}

Base rules for constants (\emph{Bool} and \emph{Num}) check whether the value is of the appropriate category and the rule for variables \emph{var} looks the type up in the environment $\Gamma$, each introducing fresh effect row variable.
To check $ \lambda $ expression with rule $\lambda$, we first introduce fresh type variable, and then check the body in the extended environment.
The arrow gets annotated with effects which may occur during evaluation of the body and the $ \lambda $ abstraction itself is returned with fresh effect row.

The recursive functions are checked using rule \emph{rec} in a fashion similar to normal functions.
First variable $x_f$ denotes function itself, while second $x_a$ its argument.
Accordingly, the environment $\Gamma$ gets extended with functional type $ t_1 \rightarrow row_1 t_2 $ for $x_f$ and argument type $ t_1 $ for $x_a$, to check the body of the function.
Afterwards the result type of body $ t $ gets unified with the result type of function $ t_2 $, same with effect row.
The whole function, as it is a value, is returned with a fresh effect row.

The rule \emph{app} for application requires the expression $e_1$ at function position to be of functional type and the expression $e_2$ at argument position to have a type that unifies with the parameter type of the function. 
All effect rows (from evaluation of $e_1$, $e_2$ and the body of the $\lambda$) must unify as well.
Inference for primitive operation call in rule \emph{prim} is deferred to auxiliary judgment \emph{check-prim}, which checks arity and argument types, returning result type and usually fresh effect row.
The rule \emph{if} for conditional expression requires the condition to be of type $ Bool $ and types of two branches to unify.
As usual all effect rows must also unify.

Operation invocation, checked by the rule \emph{op}, requires the effect row to contain operation $ op $ with type $ (t_1 \Rightarrow t_2) $ where input type $ t_1 $ is the inferred type for $ e $ and output type $ t_2 $ is fresh.
The rule \emph{lift}, checking operation lifting, prepends fresh $ op $ to the effect row of subexpression $e$.

Finally, to check handle expression with the rule \emph{handle}, we infer the type $t_1$ of the enclosed expression $e$.
Then in an environment extended with the type $t_1$ we infer the type $t_{ret}$ of the return expression.
Helper judgment \textit{infer-handlers} returns the result effect row of handlers $row_{out}$ and row marking handled effects $ row_{handled} $ with the same tail as $row_{out}$.
By unifying result row with return row and handled row with $row_1$ we ensure that effects which may occur during handling of operations, evaluation of return clause and leftovers from the inner expression are all accounted for.

\subsection{Inference for effect handlers}

\begin{figure}
  \centering
  \includegraphics{algeff-infer-handlers.pdf}
  \caption{Handlers type inference} 
  \label{fig:algeff-infer-handlers} 
\end{figure} 

List of effect handlers $ hs $ is processed right-to-left by the judgment 
$$ infer\mhyphen handlers [\Gamma , \, SN_{in} , \, t_{ret} , \, hs , \, row_{out} , \, row_{handled} , \, SN_{out}] $$
presented in \autoref{fig:algeff-infer-handlers}.
The $ t_{ret} $ is the type of the return clause, $ row_{out} $ is the combined row of effects which may occur in any handler and $ row_{handled} $ is the row of handled operations, with appropriate types.
The base case of empty list initializes both rows with the same type variable.
This way $ row_{handled} $ returned by the $infer\mhyphen handlers$ judgment will consist of all handled operations and its tail will be $ row_{out} $.
The inductive case first calculates $ row_{out} $ and $ row_{handled} $ for the tail of the list.
Then, two new fresh variables are created: $ t_v $ for the type of value passed to handler and $ t_r $ for the type of resumption parameter.
Resumption's result type is the result type of return clause as it should eventually evaluate to a value, which will be transformed by that clause.
Its effect row is the same as the result row of whole handle expression, which means that any subsequent uses of operations will be handled by this handler.
With the environment extended with $ t_v $ for the first parameter -- an argument to the handler expression and $ t_r \rightarrow row_{out} \, t_{ret} $ for second parameter -- the resumption, we check the handler expression $ e $.
We then unify the effects of evaluating $ e $ with all effects of handlers, and result type $ t_{h} $ with the return type $ t_{ret} $.
Finally we extend the handled row with the current operation $(op (t_v \Rightarrow t_r))$.

\section{Abstract machine}
The abstract machine is based on the \emph{CEK}-machine of Hillerström and Lindley\cite{Hillerstrom2016}, with the difference that during the search for operation handler, the machine must count handlers and lifts it passes.
It is similar to the abstract machine presented in extended version of Biernacki et al's \cite{Biernacki2017} as both calculi have the \emph{lift} construct.

\begin{figure}
  \centering
  \includegraphics{algeff-am-syntax.pdf}
  \caption{Abstract machine configurations}
  \label{fig:algeff-am-syntax}
\end{figure}

Possible configurations are given in \autoref{fig:algeff-am-syntax}.
Meta-variable $ C $ ranges over shapes of machine configurations, meta-variable $ V $ ranges over machine values, which are distinct from the calculus values, e.g., function values must now keep their environment $ \rho $ which maps variables to machine values.
Additionally one of the possible values is a \emph{meta-stack} which one may think of as a continuation captured during operation invocation.
Meta-variable $ \sigma $ defines normal (or pure) continuation frames and $ \Sigma $ denotes pure continuation \emph{stack}, while $ \phi $ ranges over effect frames -- \texttt{handle}, \texttt{lift} and \texttt{done} token which marks final continuation.
Meta-variable $ \kappa $ denotes meta-frame which consists of a pure stack and one effect frame.
Finally $ K $ ranges over stacks of meta-frames, which I will call \emph{meta-stack}s.
Meta-function \textit{initial-conf} in \autoref{fig:algeff-am-initial-conf} transforms an expression into initial configuration -- initializing machine with empty stack, \texttt{done} effect frame and empty meta-stack.

\begin{figure}
  \centering
  \includegraphics{algeff-am-initial-conf.pdf}
  \caption{Abstract machine -- initial configuration}
  \label{fig:algeff-am-initial-conf}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics{algeff-am-a.pdf} 
  \caption{Abstract machine -- administrative transitions}
  \label{fig:algeff-am-a}
\end{figure}

The first group of transitions depicted in \autoref{fig:algeff-am-a} performs mostly administrative functions -- capturing environment for functional values, transforming from value terms to machine values, sequencing binary operations, switching to special configuration for search of handler and a transition for returning final value.

\begin{figure}[h]
  \centering
  \includegraphics{algeff-am-b.pdf} 
  \caption{Abstract machine -- continuation building}
  \label{fig:algeff-am-b}
\end{figure}

The second group of transitions (\autoref{fig:algeff-am-b}) decomposes current expression and builds continuation by growing either stack or meta-stack.
First four rules -- for function application, operation invocation, primitive operation call and conditional expression -- all create a new pure frame and push it onto stack.
Last two transitions deal with effectful operations -- handle and lift; they build meta-stack by bundling previous effect frame with current stack into meta-frame, pushing it onto meta-stack and then installing a fresh stack and a new effect frame into the machine configuration.

\begin{figure}[h]
  \centering 
  \includegraphics{algeff-am-c.pdf}
  \caption{Abstract machine -- contractions}
  \label{fig:algeff-am-c}
\end{figure}

The third group of transitions (\autoref{fig:algeff-am-c}) perform various reductions.
The first rule looks up value of a variable in current environment, the second rule performs contraction of a normal function, by extending the environment with mapping from the function's formal parameter $ x $ to the calculated value $ V $.
The third rule reduces recursive function, extending the environment with the argument value $ V $ and the function value, allowing for recursive calls.
The last rule handling application deals with continuation resumption, by pushing the current stack and the effect frame ($ [(\sigma \ldots) \, \phi_1] $) onto the meta-stack, installing the top ($ [\Sigma \, \phi_2] $) of captured meta-stack and prepending its tail ($ (\kappa_1 \ldots) $) to the meta-stack.

\begin{figure}[h]
  \centering
  \includegraphics{algeff-am-e.pdf}
  \caption{Abstract machine -- effect handling}
  \label{fig:algeff-am-e}
\end{figure}

Transitions in the last group (\autoref{fig:algeff-am-e}) form the essence of this machine, performing all tasks related to effect handling.
First four rules search for the appropriate handler for $ op $ by maintaining a counter $ n $ which is incremented by every lift for $ op $ and decremented by every handler for $ op $.
The fifth rule matches when the counter is $ 0 $ and the handler has a case for the operation which was invoked. 
In this situation, the machine must begin evaluation of the handling expression in the environment extended both with the passed argument, and the captured continuation with current meta-frame appended.
Two last rules deal with returning values -- in case of a handler the return clause is installed, and in case of a lift its frame is simply discarded.

\chapter{Implementation}\label{ch:implementation}
This chapter discusses the implementation of the calculus and its surface language \emph{algeff}.
All the judgments and relations presented earlier were rendered by the \Redex{} library from the source files and they are the executable implementation of the calculus.
The unification algorithm used by the type inference judgment of \autoref{fig:algeff-infer} is presented in the \autoref{sec:unification}.
The \autoref{sec:file-structure} describes the file structure of the development, the \autoref{sec:front-end} discusses the language front-end and the \autoref{sec:racket-env} shows how to tie a language into the \Racket{} environment.
The surface syntax of \emph{algeff} and instructions of use are presented in \autoref{ch:manual}.

\section{Unification}\label{sec:unification}
The unification algorithm is loosely based on \cite{Pierce2002}, but instead of keeping a set of constraints to be solved, it solves sub-problems recursively.
It is implemented as a \Redex{} judgment, which takes two types, the initial substitution and returns the substitution extended with their unifier.
This judgment must also pass around the name supply token -- a natural number which is incremented every time a new type variable is created.
Variables in types are substituted lazily -- whenever algorithm encounters variable which is in domain of the substitution, it looks it up and continues unification.
The \autoref{fig:algeff-unify} shows the unification algorithm.

\begin{figure}
  \centering
  \includegraphics{algeff-unify.pdf}
  \caption{Unification algorithm}
  \label{fig:algeff-unify}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{algeff-unify-row.pdf}
  \caption{Row unification algorithm}
  \label{fig:algeff-unify-row}
\end{figure}

Terminal rules \emph{refl-var} and \emph{refl-const} handle unification of equal variables and constants respectively.
In a special case \emph{refl-var-lookup}, when both types are variables, the left variable $a_1$ is not in the domain of substitution and the right variable $a_2$ is mapped to $a_3$ which is equal to $a_1$, then they are in fact the same variable and should unify.
This corner-case arises due to lazy application of substitution to types.

The rule \emph{ext} extends substitution, when the left-hand-side type is a variable which is not in the domain of substitution.
It is important to note, that the type which is inserted into substitution is fully substituted.
This way we can maintain an invariant, that every type in the substitution has only type variables which are not in the domain of the substitution.
The last side condition, that $a$ is not a free type variable in $t$ is the occurs-check, which ensures that algorithm doesn't try to unify cyclic types, which would loop.

When the left type is a variable, in the domain of substitution, rule \emph{lookup} looks it up, and continues unification.
When the left type is not a variable, and right type is, rule \emph{flip} flips them around and continues unification.
This rule is needed because the judgment performs extension and lookup only on left variable.
The next three rules \emph{->}, \emph{=>} and \emph{List} decompose the type constructor and unify all respective sub-types.

The last rule \emph{row} unifies two rows.
It requires the left row to be non empty, and the right row not to be a variable (to ensure determinism with respect to the rule \emph{flip}).
Then it uses helper judgment \emph{unify-row} which rewrites $row_2$ such that its head is $op \, \, t_2$ and rest is bound $row_r$.
The side condition, requiring the variable at the end of (substituted) $row_1$ not to be in domain of substitution which was build by the rewriting ensures that algorithm does not try to unify rows with different labels, but same type variable in the tail (similar to occurs-check in rule \emph{ext}).

The helper judgment \emph{unify-row}, shown in \autoref{fig:algeff-unify-row} rewrites the row such that it begins with desired label $op$.
It is based on similar judgment in \cite{Leijen2005}.
There are two base cases: either the row already begins with $op$ (rule \emph{row-head}) and is decomposed and returned or the row is already a variable which is not bound by substitution (rule \emph{row-var}).
In this case the substitution is extended with a row $(op \, t \, row)$ where both $t$ and $row$ are fresh variables.

The third rule \emph{row-lookup} handles variables which are bound by the substitution, looking up the appropriate row and continuing row rewriting.
The last rule \emph{row-swap} matches rows that have a label $o1$ different to desired label $o$.
In this case, the rest of the row must rewritten recursively yielding type $t_2$ for $o$ and new tail.
The type is returned and the tail is extended with original head.

\section{File structure}\label{sec:file-structure}
The directory \texttt{lc} contains the minimal \LC{} used in the introduction.
The development -- both calculus and \Racket{} front-end is contained in the directory \texttt{algeff}.
Its sub-directory \texttt{calculus} contains source files for everything calculus-related:
\begin{itemize}
  \item The file \texttt{abstract-machine.rkt} contains the definition of abstract machine and its transitions.
  \item The file \texttt{eval.rkt} contains the reduction semantics and the \emph{free} judgment.
  \item The file \texttt{lang.rkt} contains the calculus abstract syntax definition and \emph{ftv} meta-function.
  \item The file \texttt{lib.rkt} contains various helper judgments and meta-functions.
  \item The file \texttt{type.rkt} contains the type system along with helper judgments.
  \item The file \texttt{unify.rkt} contains the implementation of the unification algorithm.
\end{itemize}
The sub-directory \texttt{lang} contains lexer and parser for the front-end.
The file \texttt{main.rkt} is evaluated by \Racket{} environment whenever a file designates \emph{algeff} as its \texttt{\#lang}.
The files \texttt{parse-only.rkt} and \texttt{tokenize-only.rkt} define sub-languages used for testing and debugging of the parser.
The sub-directory \texttt{test} contains example programs.
Finally, the file \texttt{render.rkt} renders the figures used throughout this thesis.

\section{Language front-end}\label{sec:front-end}
The language front-end consists of a tokenizer and a parser.
The tokenizer is implemented using \texttt{parser-tools} package, which provides a \texttt{lex} style lexer generator.
It defines a set of tokens and then repeatedly partitions the input stream by matching it with regular expressions.
The parser is implemented using \texttt{megaparsack} -- a library providing various parser combinators which allow for composing simple parsers together to build complex ones in monadic style.
The parser desugars some surface expressions into simpler calculus terms.
\begin{itemize}
  \item Multiple applications, which are permitted by the \emph{algeff} language are rewritten into nested single applications -- \texttt{f e$_1$ e$_2$ ... e$_n$} becomes \texttt{(app (... (app (app f e$_1$) e$_2$) ...) e$_n$)}
  \item Let-expressions \texttt{let x = exp in body} are transformed into an application: \texttt{(app (λ x body) exp)}.
  \item Letrec bindigs \texttt{letrec f x = exp in body} are similarly transformed: \texttt{(app (λ x body) (rec f x exp))}.
\end{itemize}
It also differentiates and marks variables and operation names.

\section{The \Racket{} environment}\label{sec:racket-env}
The \Racket{} environment \cite{PLT1} provides easy integration with sub-languages using its sophisticated syntax-transformer system and language specifiers.
It allows programmers to choose which language should be used to interpret the source file using the \texttt{\#lang} command at the beginning of the file.
The file must eventually be transformed into module recognized by base \Racket{}.
This way programs may be composed with modules written using various domain-specific languages.
In order to allow \Racket{} to use a new language, its developer must provide:
\begin{itemize}
  \item The reader -- a function that accepts input stream and produces abstract syntax tree of a module.
  \item The expander -- a syntax transformer that accepts the tree produced by the reader and creates a \Racket{} module, which can be interpreted by \Racket{} language.
\end{itemize}

I followed the `master recipe' described in \cite{BeautifulRacket} that specifies which components and where must be defined.
The file \texttt{main.rkt} provides an inner module \texttt{reader} and a syntax transformer \texttt{\#\%module-begin} -- the expander.
The inner module provides a function \texttt{read-syntax} which transforms the program text provided on an input port, using the parser and tokenizer described earlier, into a single calculus term represented as \Racket{} \emph{syntax object}.
It is shown in \autoref{lst:the-reader}.
\begin{listing}[H]
  \inputminted[firstline=11, lastline=19]{Racket}{../algeff/main.rkt}
  \caption{The reader}
  \label{lst:the-reader}
\end{listing}
The expander is depicted in \autoref{lst:the-expander}.
It first type checks the expression which was created by the reader, and if it succeeds, it then creates a \Racket{} module that binds several names and returns the value of reduced expression.
To create bindings, one must transform regular racket symbols into syntax objects with proper scope, which is done using \texttt{(datum->syntax ctx sym)} function which takes any syntax object and creates new one from \texttt{sym}, inheriting the scope from \texttt{ctx}.
\begin{listing}[H]
  \inputminted[firstline=29, lastline=57]{Racket}{../algeff/main.rkt}
  \caption{The expander}
  \label{lst:the-expander}
\end{listing}

\chapter{User's manual}\label{ch:manual}
\section{Requirements and installation}
The development requires a recent version of the \Racket{} environment (e.g. \texttt{7.0}), the \Redex{} library which is part of the \Racket{} distribution, and \texttt{parser-tools} and \texttt{megaparsack} libraries which can be installed using the \texttt{raco} package manager.
To install the \emph{algeff} language, enter the \texttt{algeff} directory and use \texttt{raco pkg install} command.
The \emph{algeff} should now be recognized by \Racket{} when using \texttt{\#lang} specifier.

\section{Usage and example programs}
To use the \emph{algeff} language, create a file with \texttt{.rkt} extension and set \emph{algeff} as the \texttt{\#lang}.
% \begin{itemize}
%   \item \texttt{expression} -- the source expression.
%   \item \texttt{type} -- the inferred type.
%   \item \texttt{reduce} -- a procedure which reduces the source expression using the reduction semantics.
%   \item \texttt{}
% \end{itemize}
Upon execution of the file, the program should output:
\begin{itemize}
  \item The desugared calculus term
  \item Its type
  \item Results of evaluation using the reduction semantics and the abstract machine.
\end{itemize}
An example program, calculating factorial of a number is presented in \autoref{lst:algeff-program}.
\begin{listing}[H]
  \VerbatimInput{../algeff/test/01.rkt}
  \caption{An \emph{algeff} program}
  \label{lst:algeff-program}
\end{listing}
The output produced by evaluation of the program:
\begin{verbatim}
desugared expression:
'(app
  (λ v:f (app v:f 5))
  (rec v:f v:x (if (== v:x 0) 1 (* v:x (app v:f (- v:x 1))))))

has type:
'Int

reduction result:
120

abstract machine result:
120
\end{verbatim}


\printbibliography

\end{document}
